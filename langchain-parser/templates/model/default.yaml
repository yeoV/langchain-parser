llama3.1:
  model: "Llama-3.1"
  base_url: "http://localhost:8080"
  api_key: "None"
  max_tokens: 8192
  temperature: 0